# SRLP Framework - Self-Refinement for LLM Planners

This repository contains the implementation of the SRLP (Self-Refinement for LLM Planners) framework, which enables LLMs to improve their planning capabilities through self-checking and refinement.

## Overview

The SRLP framework implements a novel approach for improving the quality of plans generated by Large Language Models (LLMs). It uses a self-checking mechanism to identify errors, inconsistencies, and constraint violations in generated plans, and then refines the plans iteratively to improve their quality.


## Running the Project

We've created several scripts to run the project:

### 1. Basic Usage

Run a simple demonstration of the framework:

```bash
python run_project.py
```

### 2. Final Demonstration

Run a comprehensive demonstration that fulfills all supervisor requirements:

```bash
python run_final_demo.py
```

This script will:
- Run 3 test scenarios (travel, cooking, project)
- Calculate and display metrics before and after refinement
- Generate visualizations for thesis inclusion
- Export results in CSV and JSON formats

### 3. Using Different LLM Providers

Run the framework with different LLM providers:

```bash
# List available providers
python run_with_llm_provider.py --list-providers

# Run with mock provider (default)
python run_with_llm_provider.py --scenario travel

# Run with OpenAI (requires API key)
python run_with_llm_provider.py --scenario cooking --provider openai --model gpt-4 --api-key YOUR_API_KEY
```

## Project Structure

The project consists of several Python scripts:

- `main.py`: Main CLI interface for the SRLP framework
- `basic_usage.py`: Basic usage examples
- `final_demonstration.py`: Comprehensive demonstration for thesis requirements
- `test_scenarios.py`: Predefined test scenarios
- `test_llm_providers.py`: Test different LLM providers
- `run_evaluation.py`: Run evaluations on scenarios
- `create_visualizations.py`: Generate visualizations for results

## Framework Components

The SRLP framework consists of several key components:

1. **Evaluator**: Comprehensive framework evaluation
2. **Refinement Engine**: Iterative refinement process
3. **Self-Checker**: Identifies errors and constraint violations
4. **Metrics Calculator**: Calculates quality metrics
5. **LLM Providers**: Integration with various LLM providers (OpenAI, Claude, LLaMA, etc.)
6. **Visualization Tools**: Charts and graphs for thesis

## Results

All results, metrics, and visualizations are saved in the `results/final_demo/` directory.